---
title: "EDB Productivity"
format: 
  dashboard:
    scrolling: true 
    nav-buttons:
      - text: Home
        icon: list
        href: https://datascientistnz.github.io/edb_productivity_dashboard
      - text: Code
        icon: github
        href: https://github.com/DataScientistNZ/edb_productivity_dashboard


editor: 
  markdown: 
    wrap: 72
    
fontsize: 13px
---

```{r, echo=F}

library(ggplot2)

my_bar_gplot <- function(dt, y_numerator, y_denominator=NULL, groupby, plot_disc_yr, flipped_axes=T) {
  
  if (!is.null(y_denominator)) {
    dt_plot <- data.table(dt)[, `:=`(metric = get(y_numerator)/get(y_denominator))
                              ][,  c("disc_yr", "edb", groupby, "metric"), with=F]
  } else {
    dt_plot <- data.table(dt)[, `:=`(metric = get(y_numerator))][,  c("disc_yr", "edb", groupby, "metric"), with=F]
  }
  
  dt_plot2 <- dt_plot[, .(metric = mean(metric)), 
                      by=c("edb", groupby)][, disc_yr := overall_period]
  dt_plot <- rbind(dt_plot, dt_plot2)
  # hack to have desired order
  dt_plot[ , my_order := match(get(groupby), sort(unique(dt_plot[[groupby]]))) * 1000000 + metric]
  
  dt_plot <- dt_plot[order(my_order)]
  
  if (flipped_axes) {
    p <- ggplot(dt_plot[disc_yr == plot_disc_yr], 
           aes(x = metric, y = reorder(edb, my_order, desc=T), fill = get(groupby))) +
      theme_minimal() + 
      theme(
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.text.y = element_text(margin = margin(r = -8))
      )
  } else {
    p <- ggplot(dt_plot[disc_yr == plot_disc_yr], 
                aes(y = metric, x = reorder(edb, my_order, desc=T), fill = get(groupby))) +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
      theme(
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(margin = margin(r = 8))
      )
  }
  
  p <- p + geom_bar(stat = "identity", alpha=0.8, position = position_dodge()) + 
    labs(
      # title = paste0(y_numerator, " / ", y_denominator, "   (", plot_disc_yr, ")"),
      x = "",
      y = "",
      fill = "")
  
  if (!is.null(y_denominator)) {
    p <- p + ggtitle(paste0(y_numerator, " / ", y_denominator, "   (", plot_disc_yr, ")"))
  } else {
    p <- p + ggtitle(paste0(y_numerator, "   (", plot_disc_yr, ")"))
  }
  
  p
}

# round all numeric values to n digits (n=3 by default)
# we do so to allow a nice display
# makes a copy of the input data by default (use copy=F if undesired)
round_all_numeric_values <- function(dt, digits=3, copy=T) {
  if (copy) dt <- data.table(dt)
  for (nm in names(dt)) {
    if (is.numeric(dt[[nm]])) {
      dt[, (nm) := round(get(nm), digits)]
    }
  }
  dt
}

# custom made display of datatable
# displaying a copy of the table (copy made in the rounding function)
my_datatable <- function(dt, ..., digits=3) {
  DT::datatable(round_all_numeric_values(dt, digits=digits), ..., 
                extensions = 'Buttons', options = list(
                  dom = 'ifrtpB', buttons = c('copy', 'csv')))
}

```

# Cobb-Douglas efficient frontier

## documentation content

### col1

<h5>EDB Productivity Benchmarking </h5>

This work is inspired by the [report](https://comcom.govt.nz/regulated-industries/electricity-lines/electricity-distributor-performance-and-data/productivity-and-efficiency-study-of-electricity-distributors?target=documents&root=356755)  delivered by CEPA for the Commerce commission that assesses the productivity of EDBs (Electricity Distribution Businesses) over time in New Zealand.

<h5>Productivity benchmarking - Relative difference to Cobb-Douglas efficient frontier approach </h5>

[TODO: introduce high level methodology]

Similar to CEPA's study, the considered inputs is the annual user charges, which is the sum of:

* Opex to operate and maintain the network.
* The flow of capital services.

The inflation indices considered to adjust the inputs are the same as in the CEPA analysis (namely 40% PPI and 60% LCI for opex, and CGPI for the flow of capital services). 

Regarding outputs, different sets have been considered in the analysis, and outputs assumptions are detailed with the presented results.

### col2

<h5>Formulas</h5>

TODO: add below content:

* generalise below Cobb Douglas Formula to n outputs and introduce notations
* Introduce benchmarked quantity as ratio of fair Cobb-Douglas cost for given inputs by actual cost 

$$
C = C_0 \cdot exp[\ \beta_1 \  ln(X_1) + \beta_2 \ ln(X_2)\ ] \cdot exp[\ \delta_t + \epsilon \ ]
$$

```{r, echo=F}

# load resources and set common variables
source(file.path(here::here(), "R", "00_cobb_douglas_benchmark.R"))
source(file.path(here::here(), "R", "00_echarts.R"))
source(file.path(here::here(), "R", "00_edb_status.R"))
options(scipen=999)

dt <- fread(file.path(here::here(), "data", "extracted_data.csv"))
```


# [validation set]

```{r, echo=F}

my_input <- "annual_charge_real"
my_outputs <- c("nb_connections", "length_circuit")
prod_idx_nm <- "prod_idx"
other_variables <- NULL
res_all <- compute_simple_econometric_benchmark(
  dt, my_input, my_outputs, other_variables=other_variables, prod_idx_nm=prod_idx_nm,
  years_filter = 2017:2021, edbs_filter=c("Alpine Energy", "Orion NZ", "Vector Lines"))
res_all <- merge(res_all, get_edb_status())  # add status feature
res_all[, rank := frankv(get(prod_idx_nm), order=-1, ties.method = "min"), by="disc_yr"]

m <- cobb_douglas_regression(dt[disc_yr %in% 2017:2021 & 
                                  edb %in% c("Alpine Energy", "Orion NZ", "Vector Lines")], 
                             my_input, my_outputs, other_variables)


```

## text into model
<h5>Relative distance to Cobb-Douglas efficient frontier approach - Validation Set </h5>
This page presents results for the Relative distance to Cobb-Douglas efficient frontier method with the validation set.
The validation set includes only 3 EDBs (Vector, Alpine and Orion) and only a few years of data (from 2017 to 2021).

The following outputs have been considered:

*  `{r} my_outputs[1]` with a sensitivity of `{r} round(m$coefficients[1+1], 3)`
*  `{r} my_outputs[2]` with a sensitivity of `{r} round(m$coefficients[2+1], 3)`


## result - all

```{r, echo=F}

eplot_line(res_all, "disc_yr", "prod_idx", groupby="edb", x_lab="") |>
  e_legend(
    orient = 'vertical',
    right = 0,
    top = "middle"
  ) |>
  e_grid(right = 200, left=50, bottom=30) |>
  e_title(paste0("Cobb Douglas rel diff to eff frontier (", "Validation Set", ")"))


```


```{r, echo=F}

my_datatable(res_all)

```


# [Outputs 1]

```{r, echo=F}

my_input <- "annual_charge_real"
my_outputs <- c("nb_connections", "length_circuit")
prod_idx_nm <- "prod_idx"
other_variables <- NULL
res_all <- compute_simple_econometric_benchmark(dt, my_input, my_outputs, 
                                                other_variables=other_variables, prod_idx_nm=prod_idx_nm)
res_all <- merge(res_all, get_edb_status())  # add status feature
res_all[, rank := frankv(get(prod_idx_nm), order=-1, ties.method = "min"), by="disc_yr"]
m <- cobb_douglas_regression(dt, my_input, my_outputs, other_variables)

```

## text into model
<h5>Relative distance to Cobb-Douglas efficient frontier approach - Validation Set </h5>
This page presents results for the Relative distance to Cobb-Douglas efficient frontier method with outputs set 1.
The following outputs have been considered:

*  `{r} my_outputs[1]` with a sensitivity of `{r} round(m$coefficients[1+1], 3)`
*  `{r} my_outputs[2]` with a sensitivity of `{r} round(m$coefficients[2+1], 3)`


## result - NonExempt and Exempt

```{r, echo=F}

my_status <- "NonExempt"
res_non_exempt <- res_all[status == my_status]
eplot_line(res_non_exempt, "disc_yr", "prod_idx", groupby="edb", x_lab="") |>
  e_legend(
    orient = 'vertical',
    right = 0,
    top = "middle"
  ) |>
  e_grid(right = 200, left=50, bottom=30) |>
  e_title(paste0("Cobb Douglas rel diff to eff frontier (", my_status, ")"))

```

```{r, echo=F}

my_status <- "Exempt"
res_exempt <- res_all[status == my_status]
eplot_line(res_exempt, "disc_yr", "prod_idx", groupby="edb", x_lab="") |>
  e_legend(
    orient = 'vertical',
    right = 0,
    top = "middle"
  ) |>
  e_grid(right = 200, left=50, bottom=30) |>
  e_title(paste0("Cobb Douglas rel diff to eff frontier (", my_status, ")"))


```

## display table of results

```{r, echo=F}

my_datatable(res_non_exempt)

```

```{r, echo=F}

my_datatable(res_exempt)

```

## Benchmarking

```{r, echo=F}

# define generic descriptors
latest_year <- max(dt$disc_yr)
overall_period <- paste0(min(dt$disc_yr), "-", latest_year)

dt_edb_info <- fread(file.path(here::here(), "data", "edb_regulatorystatus_and_peergroup.csv"))
res_all <- merge(res_all, dt_edb_info[, -"status", with=F], by="edb")

my_bar_gplot(res_all, y_numerator = "prod_idx", groupby="PAT_peergroup", plot_disc_yr=latest_year)
```

```{r, echo=F}
my_bar_gplot(res_all, y_numerator = "prod_idx", groupby="PAT_peergroup", plot_disc_yr=overall_period)
```


## Reverse Benchmarking

```{r, echo=F}

res_all[, "1 / prod_idx" := 1/prod_idx]

my_bar_gplot(res_all, y_numerator = "1 / prod_idx", groupby="PAT_peergroup", plot_disc_yr=latest_year)
```

```{r, echo=F}
my_bar_gplot(res_all, y_numerator = "1 / prod_idx", groupby="PAT_peergroup", plot_disc_yr=overall_period)
```

# [Outputs 2]

```{r, echo=F}

my_input <- "annual_charge_real"
my_outputs <- c("nb_connections", "length_circuit", "mva_underground")
prod_idx_nm <- "prod_idx"
other_variables <- NULL
res_all <- compute_simple_econometric_benchmark(dt, my_input, my_outputs, 
                                                other_variables=other_variables, prod_idx_nm=prod_idx_nm)
res_all <- merge(res_all, get_edb_status())  # add status feature
res_all[, rank := frankv(get(prod_idx_nm), order=-1, ties.method = "min"), by="disc_yr"]

m <- cobb_douglas_regression(dt, my_input, my_outputs, other_variables)

```

## text into model
<h5>Relative distance to Cobb-Douglas efficient frontier approach - Validation Set </h5>
This page presents results for the Relative distance to Cobb-Douglas efficient frontier method with outputs set 1.
The following outputs have been considered:

*  `{r} my_outputs[1]` with a sensitivity of `{r} round(m$coefficients[1+1], 3)`
*  `{r} my_outputs[2]` with a sensitivity of `{r} round(m$coefficients[2+1], 3)`
*  `{r} my_outputs[3]` with a sensitivity of `{r} round(m$coefficients[3+1], 3)`


## result - NonExempt and Exempt

```{r, echo=F}

my_status <- "NonExempt"
res_non_exempt <- res_all[status == my_status]
eplot_line(res_non_exempt, "disc_yr", "prod_idx", groupby="edb", x_lab="") |>
  e_legend(
    orient = 'vertical',
    right = 0,
    top = "middle"
  ) |>
  e_grid(right = 200, left=50, bottom=30) |>
  e_title(paste0("Cobb Douglas rel diff to eff frontier (", my_status, ")"))

```

```{r, echo=F}

my_status <- "Exempt"
res_exempt <- res_all[status == my_status]
eplot_line(res_exempt, "disc_yr", "prod_idx", groupby="edb", x_lab="") |>
  e_legend(
    orient = 'vertical',
    right = 0,
    top = "middle"
  ) |>
  e_grid(right = 200, left=50, bottom=30) |>
  e_title(paste0("Cobb Douglas rel diff to eff frontier (", my_status, ")"))


```

## display table of results

```{r, echo=F}

my_datatable(res_non_exempt)

```

```{r, echo=F}

my_datatable(res_exempt)

```

## Benchmarking

```{r, echo=F}

# define generic descriptors
latest_year <- max(dt$disc_yr)
overall_period <- paste0(min(dt$disc_yr), "-", latest_year)

dt_edb_info <- fread(file.path(here::here(), "data", "edb_regulatorystatus_and_peergroup.csv"))
res_all <- merge(res_all, dt_edb_info[, -"status", with=F], by="edb")

my_bar_gplot(res_all, y_numerator = "prod_idx", groupby="PAT_peergroup", plot_disc_yr=latest_year)
```

```{r, echo=F}
my_bar_gplot(res_all, y_numerator = "prod_idx", groupby="PAT_peergroup", plot_disc_yr=overall_period)
```


## Reverse Benchmarking

```{r, echo=F}

res_all[, "1 / prod_idx" := 1/prod_idx]

my_bar_gplot(res_all, y_numerator = "1 / prod_idx", groupby="PAT_peergroup", plot_disc_yr=latest_year)
```

```{r, echo=F}
my_bar_gplot(res_all, y_numerator = "1 / prod_idx", groupby="PAT_peergroup", plot_disc_yr=overall_period)
```
